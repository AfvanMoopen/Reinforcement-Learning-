{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observable environments\n",
    "\n",
    "In contrast to the DP chapter, we now work with environments where we don't know about all dynamics. Instead, we can interact with the environment and observe its reactions. This allows us to sample from the environment.\n",
    "\n",
    "Monte Carlo methods perform a lot of these sampling steps, and try to draw conclusions from the results. By the law of large numbers, averaging the returns from many samples converges to the true expected value.\n",
    "\n",
    "One important constraint is that the methods from this chapter only work in environments with terminal states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ObservableEnvironment:\n",
    "    def get_states(self):\n",
    "        \"\"\"\n",
    "        Return the set of possible states\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\"\n",
    "        Returns the actions that can be taken from the given state\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def execute_action(self, state, action):\n",
    "        \"\"\"\n",
    "        Returns the new state and the given reward. This does not have to\n",
    "        be deterministic\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def is_terminal_state(self, state):\n",
    "        \"\"\"\n",
    "        Returns a boolean indicating whether the state is terminal\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def sample(self, policy, state):\n",
    "        \"\"\"\n",
    "        Follows the policy until a terminal state is reached.\n",
    "        Returns a list of states, actions, and the rewards they gave\n",
    "        \"\"\"\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        while not self.is_terminal_state(state):\n",
    "            actions = self.get_possible_actions(state)\n",
    "            ps = [policy[(state, action)] for action in actions]\n",
    "            \n",
    "            action = np.random.choice(actions, p=ps)\n",
    "            \n",
    "            new_state, reward = self.execute_action(state, action)\n",
    "            \n",
    "            result.append((state, action, reward))\n",
    "            state = new_state\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BlackJack\n",
    "\n",
    "In this chapter, we try to find an optimal policy to play a simplified version of BlackJack. The game is described in detail in Example 5.1 of Sutton & Barto's book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from random import choice\n",
    "\n",
    "class BlackJack(ObservableEnvironment):\n",
    "    def __init__(self):\n",
    "        self._init_deck()\n",
    "        \n",
    "        self.HIT = 0\n",
    "        self.STICK = 1\n",
    "        \n",
    "        self.WINNING_STATE = (21, 0, 0)\n",
    "        self.LOSING_STATE = (0, 21, 0)\n",
    "        self.DRAWING_STATE = (21, 21, 0)\n",
    "        self.terminal_states = [self.WINNING_STATE, self.LOSING_STATE, self.DRAWING_STATE]\n",
    "        \n",
    "        self.WON = (self.WINNING_STATE, 1)\n",
    "        self.LOST = (self.LOSING_STATE, -1)\n",
    "        self.DRAW = (self.DRAWING_STATE, 0)\n",
    "        \n",
    "    def _init_deck(self):\n",
    "        # Ace: 1\n",
    "        # Numbers: 2 to 10\n",
    "        # Jack/Queen/King: 10\n",
    "        self.deck = range(1, 10 + 1) + [10] * 3\n",
    "        \n",
    "    def get_states(self):\n",
    "        player_state = range(12, 21 + 1)\n",
    "        dealer_state = range(1, 10 + 1)\n",
    "        usable_ace = [0, 1]\n",
    "        \n",
    "        return list(product(player_state, dealer_state, usable_ace)) + self.terminal_states\n",
    "    \n",
    "    def get_possible_actions(self, state):\n",
    "        player, dealer, usable_ace = state\n",
    "        return [self.HIT, self.STICK]\n",
    "    \n",
    "    def execute_action(self, state, action):\n",
    "        player, dealer, usable_ace = state\n",
    "        \n",
    "        if action == self.HIT:\n",
    "            new_card = self._sample_card()\n",
    "            player += new_card\n",
    "            \n",
    "            if new_card == 1:\n",
    "                usable_ace = 1\n",
    "            \n",
    "            if player > 21:\n",
    "                if usable_ace == 1:\n",
    "                    player -= 10\n",
    "                    usable_ace = 0\n",
    "                else:\n",
    "                    return self.LOST\n",
    "        elif action == self.STICK:\n",
    "            while dealer <= 17:\n",
    "                dealer += self._sample_card()\n",
    "                \n",
    "            if player < dealer <= 21:\n",
    "                return self.LOST\n",
    "            else:\n",
    "                return self.WON\n",
    "        \n",
    "        state = (player, dealer, usable_ace)\n",
    "        return state, 0\n",
    "    \n",
    "    def _sample_card(self):\n",
    "        return choice(range(1, 10 + 1))\n",
    "    \n",
    "    def is_terminal_state(self, state):\n",
    "        return state in self.terminal_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-policy\n",
    "\n",
    "On-policy methods are RL methods where the policy that we try to optimize is the same policy that we use to explore. This implies that we always keep exploring.\n",
    "\n",
    "Here, we use an $\\epsilon$-greedy approach to make sure we always keep exploring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-visit\n",
    "\n",
    "First-visit means that the value associated with a state is the mean return after the state was visited for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def print_policy(self):\n",
    "        policy = self.policy\n",
    "        \n",
    "        for state in self.env.get_states():\n",
    "            if self.env.is_terminal_state(state):\n",
    "                continue\n",
    "            \n",
    "            actions = self.env.get_possible_actions(state)                \n",
    "            QA = { action: self.Q[(state, action)] for action in actions }\n",
    "            best_action = max(QA.items(), key=itemgetter(1))[0]\n",
    "            \n",
    "            print state, best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "class OnPolicyFirstVisitMonteCarloAgent(Agent):\n",
    "    def __init__(self, env, epsilon=.05):\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.Q = {}\n",
    "        self.returns = {}\n",
    "        self.policy = {}\n",
    "        \n",
    "        for state in self.env.get_states():\n",
    "            actions = self.env.get_possible_actions(state)\n",
    "            num_actions = len(actions)\n",
    "            \n",
    "            for action in actions:\n",
    "                self.Q[(state, action)] = 0\n",
    "                self.returns[(state, action)] = []\n",
    "                self.policy[(state, action)] = 1. / num_actions\n",
    "        \n",
    "    def learn(self, num_samples):\n",
    "        for _ in range(num_samples):\n",
    "            episode = self.env.sample(self.policy, self._get_start_state())\n",
    "            \n",
    "            if len(episode) == 0:\n",
    "                continue\n",
    "            \n",
    "            self._calculate_returns(episode)\n",
    "                            \n",
    "            states, _, _ = zip(*episode)\n",
    "            for state in set(states):\n",
    "                actions = self.env.get_possible_actions(state)\n",
    "                num_actions = len(actions)\n",
    "                \n",
    "                QA = { action: self.Q[(state, action)] for action in actions }\n",
    "                best_action = max(QA.items(), key=itemgetter(1))[0]\n",
    "                \n",
    "                for action in actions:\n",
    "                    self.policy[(state, action)] = self.epsilon / num_actions\n",
    "            \n",
    "                self.policy[(state, best_action)] += 1 - self.epsilon\n",
    "    \n",
    "    def _calculate_returns(self, episode):\n",
    "        returns = {}\n",
    "        reward_after = 0\n",
    "\n",
    "        for state, action, reward in reversed(episode):\n",
    "            reward_after += reward\n",
    "            returns[(state, action)] = reward_after\n",
    "                \n",
    "        for (state, action), ret in returns.items():\n",
    "            self.returns[(state, action)].append(ret)\n",
    "            self.Q[(state, action)] = mean(self.returns[(state, action)])\n",
    "    \n",
    "    def _get_start_state(self):\n",
    "        return choice(self.env.get_states())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By learning from 500,000 samples we can learn the optimal policy. The same policy is shown in the book, but visualized a bit nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = BlackJack()\n",
    "agent = OnPolicyFirstVisitMonteCarloAgent(env)\n",
    "\n",
    "agent.learn(num_samples=500000)\n",
    "\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every-visit\n",
    "\n",
    "First-visit means that the value associated with a state is the mean return after all visits to that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OnPolicyEveryVisitMonteCarloAgent(OnPolicyFirstVisitMonteCarloAgent):\n",
    "    def _calculate_returns(self, episode):\n",
    "        returns = {}\n",
    "        reward_after = 0\n",
    "\n",
    "        for state, action, reward in reversed(episode):\n",
    "            reward_after += reward\n",
    "            returns[(state, action)] = reward_after\n",
    "            self.returns[(state, action)].append(reward_after)\n",
    "                \n",
    "        for (state, action), ret in returns.items():\n",
    "            self.Q[(state, action)] = mean(self.returns[(state, action)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this method yields the same results as the first-visit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = BlackJack()\n",
    "agent = OnPolicyEveryVisitMonteCarloAgent(env)\n",
    "\n",
    "agent.learn(num_samples=500000)\n",
    "\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy\n",
    "\n",
    "The on-policy approach has a fundamental flaw: We try to optimize a policy that also needs to keep exploring, which means it can never really be optimal.\n",
    "\n",
    "The off-policy approach tries to fix this by using a different policy just for exploration. Importance sampling is then used to make sure the results are weighted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class OffPolicyEveryVisitMonteCarloAgent(OnPolicyEveryVisitMonteCarloAgent):\n",
    "    def learn(self, num_samples):\n",
    "        self._init_u_policy()\n",
    "        \n",
    "        self.C = defaultdict(float)\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            episode = self.env.sample(self.u_policy, self._get_start_state())\n",
    "            \n",
    "            if len(episode) == 0:\n",
    "                continue\n",
    "                \n",
    "            G = 0.\n",
    "            W = 1.\n",
    "            \n",
    "            for i in reversed(range(len(episode) - 1)):\n",
    "                next_state, next_action, next_reward = episode[i + 1]\n",
    "                state, action, reward = episode[i]\n",
    "                \n",
    "                G += next_reward\n",
    "                self.C[(state, action)] += W\n",
    "                self.Q[(state, action)] += W / self.C[(state, action)] * (G - self.Q[(state, action)])\n",
    "                \n",
    "                actions = self.env.get_possible_actions(state)                \n",
    "                QA = { action: self.Q[(state, possible_action)] for possible_action in actions }\n",
    "                best_action = max(QA.items(), key=itemgetter(1))[0]\n",
    "                \n",
    "                for possible_action in actions:\n",
    "                    self.policy[(state, possible_action)] = 0\n",
    "            \n",
    "                self.policy[(state, best_action)] = 1\n",
    "                \n",
    "                if action != best_action:\n",
    "                    break\n",
    "                    \n",
    "                W *= 1. / self.u_policy[(state, action)]\n",
    "    \n",
    "    def _init_u_policy(self):\n",
    "        self.u_policy = {}\n",
    "        \n",
    "        for state in self.env.get_states():\n",
    "            actions = self.env.get_possible_actions(state)\n",
    "            num_actions = len(actions)\n",
    "            \n",
    "            for action in actions:\n",
    "                self.u_policy[(state, action)] = 1. / num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = BlackJack()\n",
    "agent = OffPolicyEveryVisitMonteCarloAgent(env)\n",
    "\n",
    "agent.learn(num_samples=500000)\n",
    "\n",
    "agent.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent.Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
