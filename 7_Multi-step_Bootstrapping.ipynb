{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-step Bootstrapping\n",
    "\n",
    "The TD methods from the last chapter all performed backups by looking one step ahead. The methods from the Monte Carlo chapter performed backups after an entire episode.\n",
    "\n",
    "Both approaches have their advantages and disadvantages, but there's no reason why we couldn't combine their ideas, by doing multi-step bootstrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windy GridWorld\n",
    "\n",
    "Just like last chapter, we'll use the windy GridWorld environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ObservableEnvironment:\n",
    "    def get_states(self):\n",
    "        \"\"\"\n",
    "        Return the set of possible states\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\"\n",
    "        Returns the actions that can be taken from the given state\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def execute_action(self, state, action):\n",
    "        \"\"\"\n",
    "        Returns the new state and the given reward. This does not have to\n",
    "        be deterministic\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def is_terminal_state(self, state):\n",
    "        \"\"\"\n",
    "        Returns a boolean indicating whether the state is terminal\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def sample(self, policy, state):\n",
    "        \"\"\"\n",
    "        Follows the policy until a terminal state is reached.\n",
    "        Returns a list of states, actions, and the rewards they gave\n",
    "        \"\"\"\n",
    "        \n",
    "        result = []\n",
    "                \n",
    "        while not self.is_terminal_state(state):\n",
    "            actions = self.get_possible_actions(state)\n",
    "            ps = [policy[(state, action)] for action in actions]\n",
    "            \n",
    "            action = np.random.choice(actions, p=ps)\n",
    "            \n",
    "            new_state, reward = self.execute_action(state, action)\n",
    "            \n",
    "            result.append((state, action, reward))\n",
    "            state = new_state\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "class WindyGridWorld(ObservableEnvironment):\n",
    "    def __init__(self, rewards, wind):\n",
    "        self.rewards = rewards\n",
    "        self.wind = wind\n",
    "        \n",
    "        n, m = rewards.shape\n",
    "        self.states = list(product(range(n), range(m)))\n",
    "        \n",
    "        self.max_down = n - 1\n",
    "        self.max_right = m - 1\n",
    "        \n",
    "        self.UP = \"UP\"\n",
    "        self.DOWN = \"DOWN\"\n",
    "        self.LEFT = \"LEFT\"\n",
    "        self.RIGHT = \"RIGHT\"\n",
    "        \n",
    "        self.ACTIONS = [self.UP, self.DOWN, self.LEFT, self.RIGHT]\n",
    "        \n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "    \n",
    "    def get_possible_actions(self, state):\n",
    "        i, j = state\n",
    "        \n",
    "        actions = []\n",
    "        \n",
    "        if i > 0:\n",
    "            actions.append(self.UP)\n",
    "            \n",
    "        if i < self.max_down:\n",
    "            actions.append(self.DOWN)\n",
    "        \n",
    "        if j > 0:\n",
    "            actions.append(self.LEFT)\n",
    "        \n",
    "        if j < self.max_right:\n",
    "            actions.append(self.RIGHT)\n",
    "            \n",
    "        return actions\n",
    "    \n",
    "    def execute_action(self, state, action):\n",
    "        i, j = state        \n",
    "        \n",
    "        if action == self.UP:\n",
    "            i -= 1\n",
    "            \n",
    "        if action == self.DOWN:\n",
    "            i += 1\n",
    "            \n",
    "        if action == self.LEFT:\n",
    "            j -= 1\n",
    "            \n",
    "        if action == self.RIGHT:\n",
    "            j += 1\n",
    "            \n",
    "        i -= self.wind[j]\n",
    "            \n",
    "        i = max(0, min(i, self.max_down))\n",
    "        j = max(0, min(j, self.max_right))\n",
    "        \n",
    "        return (i, j), self.rewards[(i, j)] - 1\n",
    "    \n",
    "    def is_terminal_state(self, state):\n",
    "        # In this GridWorld we terminate once the reward is reached\n",
    "        \n",
    "        i, j = state\n",
    "        return self.rewards[i, j] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_policy(agent, n, m):\n",
    "    lookup = {\n",
    "        \"UP\": u\"↑\",\n",
    "        \"DOWN\": u\"↓\",\n",
    "        \"LEFT\": u\"←\",\n",
    "        \"RIGHT\": u\"→\"\n",
    "    }\n",
    "    \n",
    "    result = np.zeros((n, m))\n",
    "    \n",
    "    for i in range(n):\n",
    "        result = \"\"\n",
    "        \n",
    "        for j in range(m):\n",
    "            state = (i, j)\n",
    "            actions = agent.env.get_possible_actions(state)      \n",
    "            num_actions = len(actions)\n",
    "\n",
    "            QA = { possible_action: agent.Q[(state, possible_action)] for possible_action in actions }\n",
    "            best_action = max(QA.items(), key=itemgetter(1))[0]\n",
    "            \n",
    "            result += lookup[best_action]\n",
    "        \n",
    "        print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R = np.array([\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "wind = [0, 0, 1, 1, 1, 0]\n",
    "\n",
    "env = WindyGridWorld(R, wind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $n$-step Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import choice\n",
    "from operator import itemgetter\n",
    "from Queue import Queue\n",
    "\n",
    "class SarsaAgent:\n",
    "    def __init__(self, env, num_steps=1, discount_factor=1., alpha=.4, epsilon=.05):\n",
    "        self.env = env\n",
    "        self.num_steps = 1\n",
    "        self.discount_factor = discount_factor\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.Q = {}\n",
    "        self.policy = {}\n",
    "        \n",
    "        for state in env.get_states():\n",
    "            actions = env.get_possible_actions(state)\n",
    "            num_actions = len(actions)\n",
    "            \n",
    "            for action in actions:\n",
    "                self.Q[(state, action)] = 0.\n",
    "                self.policy[(state, action)] = 1. / num_actions\n",
    "    \n",
    "    def learn(self, num_samples, initial_state):\n",
    "        for _ in range(num_samples):\n",
    "            S = initial_state\n",
    "            A = self._choose_action(S)\n",
    "            \n",
    "            t = 0\n",
    "            rewards = Queue()\n",
    "            reward_sum = 0\n",
    "            \n",
    "            while not self.env.is_terminal_state(S):                \n",
    "                S_next, R = self.env.execute_action(S, A)\n",
    "                A_next = self._choose_action(S_next)\n",
    "                \n",
    "                rewards.put((R, S, A))\n",
    "                reward_sum += R\n",
    "                \n",
    "                if t >= self.num_steps:\n",
    "                    old_reward, S_old, A_old = rewards.get()\n",
    "\n",
    "                    # SA R SA\n",
    "                    self.Q[(S_old, A_old)] += self.alpha * (reward_sum + self.discount_factor * self.Q[(S_next, A_next)] - self.Q[(S_old, A_old)])\n",
    "\n",
    "                    reward_sum -= old_reward\n",
    "                    \n",
    "                self._update_policy()\n",
    "                \n",
    "                t += 1\n",
    "                S, A = S_next, A_next\n",
    "                \n",
    "    def _inner_learn(self, S, A):\n",
    "        return S_next, A_next\n",
    "                \n",
    "    def _choose_action(self, S):\n",
    "        actions = self.env.get_possible_actions(S)\n",
    "        \n",
    "        ps = [self.policy[(S, A)] for A in actions]\n",
    "        A = np.random.choice(actions, p=ps)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def _update_policy(self):\n",
    "        for state in self.env.get_states():\n",
    "            actions = self.env.get_possible_actions(state)      \n",
    "            num_actions = len(actions)\n",
    "            \n",
    "            best_action = self._find_best_action(state)\n",
    "            \n",
    "            for action in actions:\n",
    "                self.policy[(state, action)] = self.epsilon / num_actions\n",
    "                \n",
    "            self.policy[(state, best_action)] += 1 - self.epsilon\n",
    "            \n",
    "    def _find_best_action(self, state):\n",
    "        QA = { action: self.Q[(state, action)] for action in self.env.get_possible_actions(state) }\n",
    "        return max(QA.items(), key=itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-step Sarsa agent with $n = 1$ is the same as the one from the previous chapter. It also arrives at the same final policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→→→→→↓\n",
      "→→→↓↓↓\n",
      "→→→→→←\n"
     ]
    }
   ],
   "source": [
    "sarsa_agent = SarsaAgent(env, num_steps=1)\n",
    "sarsa_agent.learn(num_samples=2000, initial_state=(0, 0))\n",
    "draw_policy(sarsa_agent, 3, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, agents that take a larger number of steps arrive at the same policy or an equivalent optimal policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→→→→→↓\n",
      "→→↑↓↓↓\n",
      "→→→→→←\n"
     ]
    }
   ],
   "source": [
    "sarsa_agent = SarsaAgent(env, num_steps=2)\n",
    "sarsa_agent.learn(num_samples=2000, initial_state=(0, 0))\n",
    "draw_policy(sarsa_agent, 3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→→→→→↓\n",
      "→→→↓↓↓\n",
      "→→→→→←\n"
     ]
    }
   ],
   "source": [
    "sarsa_agent = SarsaAgent(env, num_steps=3)\n",
    "sarsa_agent.learn(num_samples=2000, initial_state=(0, 0))\n",
    "draw_policy(sarsa_agent, 3, 6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
